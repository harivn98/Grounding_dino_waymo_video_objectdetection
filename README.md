This project demonstrates text-based object detection using the Grounding DINO model. It allows detecting objects in images and videos simply by providing text prompts such as “car,” “person,” or “traffic light.” The model combines vision and language understanding to recognize and localize objects without the need for retraining.

The implementation is done in Python using libraries like Transformers, PyTorch, and OpenCV. It supports both single image detection and video frame analysis, automatically drawing bounding boxes and confidence scores around detected objects. The results can be visualized directly or saved as annotated videos.

This project serves as an example of how vision-language models can perform zero-shot object detection, which is useful for automation, surveillance, and computer vision research. It shows the power of connecting natural language with visual understanding in a simple and practical way.
